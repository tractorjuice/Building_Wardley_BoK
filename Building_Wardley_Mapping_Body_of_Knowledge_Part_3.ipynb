{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tractorjuice/Building_Wardley_BoK/blob/main/Building_Wardley_Mapping_Body_of_Knowledge_Part_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07c1e3b9"
      },
      "source": [
        "# Build an AI Body of Knowledge Using Langchain & OpenAI\n",
        "## Part 3, create the vector database\n",
        "\n",
        "This example shows how to create and query an internal knowledge base using ChatGPT.\n",
        "\n",
        "This does not requires a GPU runtime."
      ],
      "id": "07c1e3b9"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZMGuOEMrG2V"
      },
      "source": [
        "### Runtime Checks"
      ],
      "id": "pZMGuOEMrG2V"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b95p5xdwLUSC"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "  gpu_info = !nvidia-smi\n",
        "except:\n",
        "  print('No GPU')\n",
        "else:\n",
        "  gpu_info = '\\n'.join(gpu_info)\n",
        "  if gpu_info.find('failed') >= 0:\n",
        "    print('Not connected to a GPU')\n",
        "  print(gpu_info)"
      ],
      "id": "b95p5xdwLUSC"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hi64puI8Lf--"
      },
      "outputs": [],
      "source": [
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ],
      "id": "Hi64puI8Lf--"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yxwAfAD9qm2e"
      },
      "source": [
        "## Set Up\n"
      ],
      "id": "yxwAfAD9qm2e"
    },
    {
      "cell_type": "code",
      "source": [
        "# Make sure you have a settings.ini file in your directory with the required searches and/or playlists\n",
        "\n",
        "\"\"\"\n",
        "; System settings\n",
        "[System]\n",
        "KB_FOLDER = \"/content/gdrive/MyDrive/AI/WardleyKB\"\n",
        "\n",
        "[YouTube]\n",
        "YT_PLAYLISTS = \"                                     \"\n",
        "YT_SEARCHES = \"DataOps\", \"FinOps\", \"MLOps\"\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "v_akia_618Xr"
      },
      "id": "v_akia_618Xr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rac3kWMzqm2f"
      },
      "source": [
        "Mount Google Drive"
      ],
      "id": "rac3kWMzqm2f"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GRXVEdhrqm2f"
      },
      "outputs": [],
      "source": [
        "# Mount Google Drive\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/gdrive')\n",
        "except Exception as e:\n",
        "    print(f\"Failed to mount Google Drive. Reason: {e}\")"
      ],
      "id": "GRXVEdhrqm2f"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get required secrets and setup any keys"
      ],
      "metadata": {
        "id": "wACjZwntCuxq"
      },
      "id": "wACjZwntCuxq"
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')"
      ],
      "metadata": {
        "id": "_Xq6IfmmCtPy"
      },
      "id": "_Xq6IfmmCtPy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check / create directory structure"
      ],
      "metadata": {
        "id": "0lTX1LEtQzyp"
      },
      "id": "0lTX1LEtQzyp"
    },
    {
      "cell_type": "code",
      "source": [
        "debug = False  # set this to True if you only want the first 5 files\n",
        "SETTINGS = \"/content/gdrive/MyDrive/AI/WardleyKB/settings.ini\""
      ],
      "metadata": {
        "id": "0tdU961ydzAb"
      },
      "execution_count": null,
      "outputs": [],
      "id": "0tdU961ydzAb"
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the settings.ini file to get key configuration\n",
        "import configparser\n",
        "config = configparser.ConfigParser()\n",
        "config.read(SETTINGS)"
      ],
      "metadata": {
        "id": "HJGzkg1q42r_"
      },
      "id": "HJGzkg1q42r_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BPsvKKXIo3f2"
      },
      "outputs": [],
      "source": [
        "import os, datetime\n",
        "\n",
        "# Get the current time at the start of the program and format as a string\n",
        "start_time = datetime.datetime.now()\n",
        "start_time_str = start_time.strftime(\"%Y%m%d%H%M\")\n",
        "\n",
        "# Read the settings.ini file\n",
        "try:\n",
        "    KB_FOLDER = config.get('System', 'KB_FOLDER').strip('\"')\n",
        "except configparser.NoOptionError:\n",
        "    print(\"Missing configuration key.\")\n",
        "except Exception as e:\n",
        "    print(f\"Failed to read configuration. Reason: {e}\")\n",
        "\n",
        "print(\"Root Folder: \", KB_FOLDER)\n",
        "YT = os.path.join(KB_FOLDER, \"youtube\")  # All YouTube files\n",
        "YT_DATASTORE = os.path.join(YT, \"datastore\")  # Sub-directory for YouTube FAISS datastore files\n",
        "YT_VIDEOS = os.path.join(YT, \"videos\")  # Sub-directory for audio files\n",
        "YT_AUDIO = os.path.join(YT_VIDEOS, \"audio\")  # Sub-directory for audio files\n",
        "YT_TRANSCRIPTS = os.path.join(YT_VIDEOS, \"transcripts\")  # Sub-directory for transcripts of audio files\n",
        "YT_TRANSCRIPTS_TEXT = os.path.join(YT_TRANSCRIPTS, \"full_text\")  # Sub-directory for text of audio files\n",
        "YT_TRANSCRIPTS_WHISPER = os.path.join(YT_TRANSCRIPTS, \"whisper_chunks\")  # Sub-directory for Whisper chunks of audio files\n",
        "YT_TRANSCRIPTS_WHISPER_DISTIL = os.path.join(YT_TRANSCRIPTS, \"distil_whisper_chunks\")  # Sub-directory for Distil Whisper chunks of audio files\n",
        "YT_TRANSCRIPTS_COMBINED = os.path.join(YT_TRANSCRIPTS, \"combined_transcripts\")  # Sub-directory for books FAIS datastore file\n",
        "YT_TRANSCRIPTS_DATASTORE = os.path.join(YT_TRANSCRIPTS, \"datastore\")  # Sub-directory for books FAISS datastore file\n",
        "PODCAST = os.path.join(KB_FOLDER, \"podcast\")  # Sub-directory for YouTube FAIS datastore files\n",
        "PODCAST_DATASTORE = os.path.join(PODCAST, \"datastore\")  # Sub-directory for YouTube FAIS datastore files\n",
        "PODCAST_AUDIO = os.path.join(PODCAST, \"audio\")  # Sub-directory for YouTube FAIS datastore files\n",
        "PODCAST_TRANSCRIPTS = os.path.join(PODCAST, \"transcripts\")  # Sub-directory for YouTube FAIS datastore files\n",
        "MAPS = os.path.join(KB_FOLDER, \"maps\")  # Sub-directory for research 2022 files\n",
        "MAPS_DATASTORE = os.path.join(MAPS, \"datastore\")  # Sub-directory for maps FAIS datastore files\n",
        "\n",
        "directories = [\n",
        "    YT,\n",
        "    YT_DATASTORE,\n",
        "    YT_VIDEOS,\n",
        "    YT_AUDIO,\n",
        "    YT_TRANSCRIPTS,\n",
        "    YT_TRANSCRIPTS_TEXT,\n",
        "    YT_TRANSCRIPTS_WHISPER,\n",
        "    YT_TRANSCRIPTS_WHISPER_DISTIL,\n",
        "    YT_TRANSCRIPTS_COMBINED,\n",
        "    YT_TRANSCRIPTS_DATASTORE,\n",
        "    PODCAST,\n",
        "    PODCAST_DATASTORE,\n",
        "    PODCAST_AUDIO,\n",
        "    PODCAST_TRANSCRIPTS,\n",
        "    MAPS,\n",
        "    MAPS_DATASTORE\n",
        "    ]\n",
        "\n",
        "for directory in directories:\n",
        "    print (\"Folders    : \", directory)\n",
        "    if not os.path.exists(directory):\n",
        "        try:\n",
        "            os.makedirs(directory)\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to create {directory}. Reason: {e}\")\n"
      ],
      "id": "BPsvKKXIo3f2"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install required dependencies"
      ],
      "metadata": {
        "id": "R0y06r7V31jK"
      },
      "id": "R0y06r7V31jK"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U langchain\n",
        "!pip install -q -U langchain-community\n",
        "!pip install -q -U langchain_openai"
      ],
      "metadata": {
        "id": "8z_hmauT301v"
      },
      "id": "8z_hmauT301v",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set up OpenAI Model and necessary variables"
      ],
      "metadata": {
        "id": "sIp0lue73-Yo"
      },
      "id": "sIp0lue73-Yo"
    },
    {
      "cell_type": "code",
      "source": [
        "#MODEL = \"gpt-3.5-turbo-16k\" # Legacy\n",
        "#MODEL = \"gpt-3.5-turbo-1106\" # Current model\n",
        "MODEL = \"gpt-3.5-turbo-0125\" # Latest model\n",
        "#MODEL = \"gpt-4-0125-preview\" Latest model"
      ],
      "metadata": {
        "id": "zzBcSJT63-_j"
      },
      "id": "zzBcSJT63-_j",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialise preferred vectorstore"
      ],
      "metadata": {
        "id": "p4P-ETfv4KDP"
      },
      "id": "p4P-ETfv4KDP"
    },
    {
      "cell_type": "code",
      "source": [
        "vs = 'PineconeServerless' # Set to 'Pinecone' or 'FAISS' for the vector datbase. If using FAISS, no GPU required"
      ],
      "metadata": {
        "id": "HdjrAZfG34ok"
      },
      "id": "HdjrAZfG34ok",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if vs == 'Pinecone':\n",
        "    !pip install -q pinecone-client\n",
        "    from langchain.vectorstores import Pinecone\n",
        "    from tqdm.auto import tqdm\n",
        "    import pinecone\n",
        "\n",
        "    # initialize pinecone\n",
        "    pinecone.init(\n",
        "        api_key = userdata.get('PINECONE_API_KEY'), # find at app.pinecone.io\n",
        "        environment=\"gcp-starter\"  # next to api key in console\n",
        "        )\n",
        "\n",
        "    index_name = \"knowledge\" # Put your Pincecone index name here\n",
        "    name_space = \"wardleykb\" # Put your Pincecone namespace here\n",
        "\n",
        "elif vs == 'PineconeServerless':\n",
        "    !pip install -q -U pinecone-client\n",
        "    from pinecone import Pinecone\n",
        "    api_key = userdata.get('PINECONE_API_KEY')\n",
        "    import os\n",
        "    os.environ[\"PINECONE_API_KEY\"] = api_key\n",
        "\n",
        "    # initialize pinecone\n",
        "    pc = Pinecone(api_key=api_key)\n",
        "\n",
        "    from pinecone import ServerlessSpec, PodSpec\n",
        "    spec = ServerlessSpec(cloud='AWS', region='us-west-2')\n",
        "    index_name = 'wardleykb'\n",
        "\n",
        "elif vs == 'FAISS':\n",
        "    !pip install -q faiss-cpu\n",
        "    from langchain.vectorstores import FAISS\n",
        "\n",
        "elif vs == \"CHROMA\":\n",
        "    !pip install chromadb\n",
        "    from langchain.vectorstores import Chroma\n",
        "\n"
      ],
      "metadata": {
        "id": "JZ3Xu5tQ4NdP"
      },
      "id": "JZ3Xu5tQ4NdP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1mHZRgDKXv1r"
      },
      "source": [
        "# Build the datastore"
      ],
      "id": "1mHZRgDKXv1r"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Split text and create chunks, create metadata and upsert embeddings to vectorstore"
      ],
      "metadata": {
        "id": "szx2cWCiusZh"
      },
      "id": "szx2cWCiusZh"
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain_openai import OpenAIEmbeddings"
      ],
      "metadata": {
        "id": "-D1uKLEKLsz6"
      },
      "id": "-D1uKLEKLsz6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Upsert video embeddings to preferred vector store"
      ],
      "metadata": {
        "id": "vfvhTXHYu0xq"
      },
      "id": "vfvhTXHYu0xq"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UHviRb3WmbYW"
      },
      "outputs": [],
      "source": [
        "docs = []\n",
        "metadatas = []\n",
        "embedding_data = []\n",
        "unique_video_ids = []\n",
        "transcriptions = []\n",
        "counter = 0\n",
        "texts = []\n",
        "start_times = []\n",
        "\n",
        "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0, separator=\"\\n\")\n",
        "embeddings = OpenAIEmbeddings()\n",
        "from langchain.vectorstores import Pinecone # Flip over to LanhChain Pinecone\n",
        "embeddings_file = f'{YT_DATASTORE}/embeddings.json'\n",
        "\n",
        "with open(f'{YT_VIDEOS}/videos.txt', 'r') as file:\n",
        "    for line in file:\n",
        "        # Remove linebreak which is the last character of the string\n",
        "        curr_place = line[:-1]\n",
        "        # Add item to the list\n",
        "        unique_video_ids.append(curr_place)\n",
        "\n",
        "total_videos = len(unique_video_ids)\n",
        "\n",
        "for video_id in unique_video_ids:\n",
        "    counter = counter + 1\n",
        "    transcript_filename = f'{YT_TRANSCRIPTS_WHISPER_DISTIL}/' + video_id + '_large.txt'\n",
        "    url = \"https://www.youtube.com/watch?v=\" + video_id\n",
        "    try:\n",
        "        file = open(transcript_filename, 'r')\n",
        "    except:\n",
        "        print(f'{counter} of {total_videos}: File does not exist {transcript_filename}, skipping.')\n",
        "    else:\n",
        "        print(f'{counter} of {total_videos}: Loading {transcript_filename} ......\\n')\n",
        "        transcription = json.load(file)\n",
        "        texts = []\n",
        "        start_times = []\n",
        "        docs = []\n",
        "        metadatas = []\n",
        "\n",
        "        for chunk in transcription['chunks']:\n",
        "            if chunk['timestamp'][0] is not None:\n",
        "                text = chunk['text']\n",
        "                start = int(chunk['timestamp'][0])\n",
        "                texts.append(text)\n",
        "                start_times.append(start)\n",
        "\n",
        "        # Load the JSON file\n",
        "        video_id_no_ext = video_id.replace('.webm', '')\n",
        "        with open(f'{YT_AUDIO}/{video_id_no_ext}.info.json', \"r\") as file:\n",
        "            video_info = json.load(file)\n",
        "\n",
        "        # Extracting general video details from the JSON data\n",
        "        general_video_details = {\n",
        "            #\"ID\": video_info.get(\"id\", None), # Try and fix error with FAISS. ID Error\n",
        "            \"Title\": video_info.get(\"fulltitle\", None),\n",
        "            \"Description\": video_info.get(\"description\", None),\n",
        "            \"Duration\": video_info.get(\"duration_string\", None),\n",
        "            \"Uploader\": video_info.get(\"uploader\", None),\n",
        "            \"Upload Date\": video_info.get(\"upload_date\", None),\n",
        "            \"View Count\": video_info.get(\"view_count\", None),\n",
        "            \"Like Count\": video_info.get(\"like_count\", None),\n",
        "            \"Dislike Count\": video_info.get(\"dislike_count\", None),\n",
        "            \"Average Rating\": video_info.get(\"average_rating\", None),\n",
        "        }\n",
        "\n",
        "        # Filtering out any None values for cleaner presentation\n",
        "        general_video_details = {k: v for k, v in general_video_details.items() if v is not None}\n",
        "\n",
        "        video_title = general_video_details.get(\"Title\")\n",
        "        video_description = general_video_details.get(\"Description\")\n",
        "        video_duration = general_video_details.get(\"Duration\")\n",
        "        video_uploader = general_video_details.get(\"Uploader\")\n",
        "        video_upload_date = general_video_details.get(\"Upload Date\")\n",
        "        video_view_count = general_video_details.get(\"View Count\")\n",
        "        video_like_count = general_video_details.get(\"Like Count\")\n",
        "        video_dislike_count = general_video_details.get(\"Dislike Count\")\n",
        "        video_average_rating = general_video_details.get(\"Average Rating\")\n",
        "\n",
        "        # Now, you can use these variables throughout your code\n",
        "        print(\"Title      :\",video_title)\n",
        "        print(\"Uploader   :\", video_uploader)\n",
        "        print(\"Duration   :\", video_duration, \"\\n\")\n",
        "\n",
        "        for i, d in enumerate(texts):\n",
        "            splits = text_splitter.split_text(d)\n",
        "            docs.extend(splits)\n",
        "            metadatas.extend([{\n",
        "                key: value for key, value in {\n",
        "                    \"source\": \"YouTube\",\n",
        "                    \"source_video\": video_id,\n",
        "                    \"start_time\": start_times[i],\n",
        "                    \"title\": video_title,\n",
        "                    \"author\": video_uploader,\n",
        "                    \"description\": video_description,\n",
        "                    \"upload_date\": video_upload_date,\n",
        "                    \"view_count\": video_view_count,\n",
        "                    \"like_count\": video_like_count,\n",
        "                    \"dislike_count\": video_dislike_count,\n",
        "                    \"average_rating\": video_average_rating,\n",
        "                }.items() if value is not None # Remove 'null' values, as not supported by Pinecone Serverless\n",
        "            }])\n",
        "\n",
        "        if vs == 'Pinecone':\n",
        "            try:\n",
        "                vector_store = Pinecone.from_texts(docs, embeddings, metadatas=metadatas, index_name=index_name, namespace=name_space)\n",
        "            except:\n",
        "                print(\"Error upserting data into the vectorstore\\n\")\n",
        "        elif vs == 'PineconeServerless':\n",
        "            try:\n",
        "                print(\"Contents   :\",docs)\n",
        "                print(\"             Saving data to the serverless vectorstore\")\n",
        "                vector_store = Pinecone.from_texts(docs, embeddings, metadatas=metadatas, index_name=index_name)\n",
        "                print(\"             Vectorstore save complete\")\n",
        "            except:\n",
        "                print(\"              Error upserting data into the vectorstore\\n\")\n",
        "        elif vs == \"FAISS\":\n",
        "            try:\n",
        "                vector_store = FAISS.from_texts(docs, embeddings, metadatas=metadatas)\n",
        "                if os.path.exists(f\"{YT_DATASTORE}/index.faiss\"):\n",
        "                    existing_index=FAISS.load_local(f\"{YT_DATASTORE}\", embeddings)\n",
        "                    existing_index.merge_from(vector_store)\n",
        "                    existing_index.save_local(f\"{YT_DATASTORE}\")\n",
        "                else:\n",
        "                    vector_store.save_local(f\"{YT_DATASTORE}\") # Download the files `$DATA_STORE_DIR/index.faiss` and `$DATA_STORE_DIR/index.pkl` to local\n",
        "\n",
        "            except:\n",
        "                print(\"Error upserting data into the vectorstore\\n\")\n",
        "        elif vs == \"CHROMA\":\n",
        "            try:\n",
        "                vector_store = Chroma.from_texts(docs, embeddings, metadatas=metadatas, persist_directory=YT_DATASTORE)\n",
        "            except:\n",
        "                print(\"Error upserting data into the vectorstore\\n\")\n",
        "\n",
        "        print('-' * 50)  # Just a separator line for clarity"
      ],
      "id": "UHviRb3WmbYW"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuClass": "premium",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.1"
    },
    "vscode": {
      "interpreter": {
        "hash": "b1677b440931f40d89ef8be7bf03acb108ce003de0ac9b18e8d43753ea2e7103"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}