{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM+jE8QhZJz1cXDLtmE7+xs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tractorjuice/Building_Wardley_BoK/blob/main/Building_AI_Body_of_Knowledge_Part_17_Websites.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yxwAfAD9qm2e"
      },
      "source": [
        "## Set Up\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rac3kWMzqm2f"
      },
      "source": [
        "Mount Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GRXVEdhrqm2f"
      },
      "outputs": [],
      "source": [
        "# Mount Google Drive\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/gdrive')\n",
        "except Exception as e:\n",
        "    print(f\"Failed to mount Google Drive. Reason: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get required secrets and setup any keys"
      ],
      "metadata": {
        "id": "wACjZwntCuxq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')"
      ],
      "metadata": {
        "id": "_Xq6IfmmCtPy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check / create directory structure"
      ],
      "metadata": {
        "id": "0lTX1LEtQzyp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "debug = False  # set this to True if you only want the first 5 files\n",
        "\n",
        "SETTINGS = \"/content/gdrive/MyDrive/AI/WardleyMapsKB/settings.ini\""
      ],
      "metadata": {
        "id": "0tdU961ydzAb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the settings.ini file to get key configuration\n",
        "import configparser\n",
        "config = configparser.ConfigParser()\n",
        "config.read(SETTINGS)"
      ],
      "metadata": {
        "id": "HJGzkg1q42r_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BPsvKKXIo3f2"
      },
      "outputs": [],
      "source": [
        "import os, datetime\n",
        "\n",
        "# Get the current time at the start of the program and format as a string\n",
        "start_time = datetime.datetime.now()\n",
        "start_time_str = start_time.strftime(\"%Y%m%d%H%M\")\n",
        "\n",
        "# Read the settings.ini file\n",
        "try:\n",
        "    KB_FOLDER = config.get('System', 'KB_FOLDER').strip('\"')\n",
        "except configparser.NoOptionError:\n",
        "    print(\"Missing configuration key.\")\n",
        "except Exception as e:\n",
        "    print(f\"Failed to read configuration. Reason: {e}\")\n",
        "\n",
        "YT = os.path.join(KB_FOLDER, \"youtube\")  # All YouTube files\n",
        "YT_DATASTORE = os.path.join(YT, \"datastore\")  # Sub-directory for YouTube FAISS datastore files\n",
        "YT_VIDEOS = os.path.join(YT, \"videos\")  # Sub-directory for audio files\n",
        "YT_AUDIO = os.path.join(YT_VIDEOS, \"audio\")  # Sub-directory for audio files\n",
        "YT_TRANSCRIPTS = os.path.join(YT_VIDEOS, \"transcripts\")  # Sub-directory for transcripts of audio files\n",
        "YT_TRANSCRIPTS_TEXT = os.path.join(YT_TRANSCRIPTS, \"full_text\")  # Sub-directory for text of audio files\n",
        "YT_TRANSCRIPTS_WHISPER = os.path.join(YT_TRANSCRIPTS, \"whisper_chunks\")  # Sub-directory for Whisper chunks of audio files\n",
        "YT_TRANSCRIPTS_WHISPER_DISTIL = os.path.join(YT_TRANSCRIPTS, \"distil_whisper_chunks\")  # Sub-directory for Distil Whisper chunks of audio files\n",
        "YT_TRANSCRIPTS_COMBINED = os.path.join(YT_TRANSCRIPTS, \"combined_transcripts\")  # Sub-directory for books FAIS datastore file\n",
        "YT_TRANSCRIPTS_DATASTORE = os.path.join(YT_TRANSCRIPTS, \"datastore\")  # Sub-directory for books FAISS datastore file\n",
        "\n",
        "PODCAST = os.path.join(KB_FOLDER, \"podcast\")  # Sub-directory for YouTube FAIS datastore files\n",
        "PODCAST_DATASTORE = os.path.join(PODCAST, \"datastore\")  # Sub-directory for YouTube FAIS datastore files\n",
        "PODCAST_AUDIO = os.path.join(PODCAST, \"audio\")  # Sub-directory for YouTube FAIS datastore files\n",
        "PODCAST_TRANSCRIPTS = os.path.join(PODCAST, \"transcripts\")  # Sub-directory for YouTube FAIS datastore files\n",
        "\n",
        "MAPS = os.path.join(KB_FOLDER, \"maps\")  # Sub-directory for research 2022 files\n",
        "MAPS_DATASTORE = os.path.join(MAPS, \"datastore\")  # Sub-directory for maps FAIS datastore files\n",
        "\n",
        "WEBSITES_DATA = os.path.join(KB_FOLDER, \"websites\")\n",
        "WEBSITES_CONTENTS = os.path.join(WEBSITES_DATA, \"contents\")\n",
        "WEBSITES_DATASTORE = os.path.join(WEBSITES_DATA, \"datastore\")\n",
        "\n",
        "directories = [\n",
        "    YT,\n",
        "    YT_DATASTORE,\n",
        "    YT_VIDEOS,\n",
        "    YT_AUDIO,\n",
        "    YT_TRANSCRIPTS,\n",
        "    YT_TRANSCRIPTS_TEXT,\n",
        "    YT_TRANSCRIPTS_WHISPER,\n",
        "    YT_TRANSCRIPTS_WHISPER_DISTIL,\n",
        "    YT_TRANSCRIPTS_COMBINED,\n",
        "    YT_TRANSCRIPTS_DATASTORE,\n",
        "    PODCAST,\n",
        "    PODCAST_DATASTORE,\n",
        "    PODCAST_AUDIO,\n",
        "    PODCAST_TRANSCRIPTS,\n",
        "    MAPS,\n",
        "    MAPS_DATASTORE,\n",
        "    WEBSITES_DATA,\n",
        "    WEBSITES_CONTENTS,\n",
        "    WEBSITES_DATASTORE\n",
        "    ]\n",
        "\n",
        "for directory in directories:\n",
        "    print (directory)\n",
        "    if not os.path.exists(directory):\n",
        "        try:\n",
        "            os.makedirs(directory)\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to create {directory}. Reason: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install required dependencies"
      ],
      "metadata": {
        "id": "R0y06r7V31jK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U langchain-core\n",
        "!pip install -q -U langchain-community\n",
        "!pip install -q -U langchain-openai\n",
        "!pip install -q -U langchain\n",
        "!pip install -q -U fake-useragent # Try to give a random fake user header for html calls"
      ],
      "metadata": {
        "id": "8z_hmauT301v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set up OpenAI Model and necessary variables"
      ],
      "metadata": {
        "id": "sIp0lue73-Yo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#MODEL = \"gpt-3.5-turbo-16k\" # Legacy\n",
        "MODEL = \"gpt-3.5-turbo-1106\" # Current model\n",
        "#MODEL = \"gpt-3.5-turbo-0125\" # Latest model\n",
        "#MODEL = \"gpt-4-0125-preview\" Latest model"
      ],
      "metadata": {
        "id": "zzBcSJT63-_j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Scrape Websites"
      ],
      "metadata": {
        "id": "_hh3B0DdD0EH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V2ivF002-dxW"
      },
      "outputs": [],
      "source": [
        "!pip install -q -U playwright beautifulsoup4 html2text\n",
        "!playwright install\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Read** URLs configuration file"
      ],
      "metadata": {
        "id": "L4j8r9RTCrEX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the settings.ini file\n",
        "try:\n",
        "    website_urls = config.get('Websites', 'URLS').strip('\"')\n",
        "    urls = [url.strip().replace('\"', '') for url in website_urls.split(',')]\n",
        "except configparser.NoOptionError:\n",
        "    print(\"Missing configuration key.\")\n",
        "except Exception as e:\n",
        "    print(f\"Failed to read configuration. Reason: {e}\")\n",
        "\n",
        "print(urls)"
      ],
      "metadata": {
        "id": "bfAaaUBiCvEV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urljoin, urlparse\n",
        "\n",
        "headers = {\"user-agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.182 Safari/537.36\"}\n",
        "\n",
        "def get_first_level_urls(site_url):\n",
        "    try:\n",
        "        # Send a request to the website\n",
        "        response = requests.get(site_url, headers=headers, timeout=5)\n",
        "        response.raise_for_status()\n",
        "\n",
        "        # Parse the content with BeautifulSoup\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "        # Extract all links\n",
        "        links = set()\n",
        "        for a_tag in soup.find_all('a', href=True):\n",
        "            url = urljoin(site_url, a_tag['href'])\n",
        "            parsed_url = urlparse(url)\n",
        "            # Check if the link is a first-level URL\n",
        "            if parsed_url.netloc == urlparse(site_url).netloc and parsed_url.path.count('/') == 1:\n",
        "                links.add(url)\n",
        "\n",
        "        return list(links)\n",
        "\n",
        "    except requests.RequestException as e:\n",
        "        print(f\"Error during requests to {site_url} : {str(e)}\")\n",
        "\n",
        "# Example usage\n",
        "site_url = urls[0]\n",
        "first_level_urls = get_first_level_urls(site_url)\n",
        "for pages in first_level_urls:\n",
        "    print(pages)\n"
      ],
      "metadata": {
        "id": "Fgk3NfQSKvzg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import json\n",
        "from langchain_community.document_loaders import AsyncHtmlLoader\n",
        "from langchain.document_transformers import Html2TextTransformer\n",
        "from aiohttp.client_exceptions import TooManyRedirects, InvalidURL\n",
        "\n",
        "# Setup OpenAI embeddings for vector database\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "embeddings = OpenAIEmbeddings()\n",
        "\n",
        "def is_valid_url(url):\n",
        "    return url.startswith('http://') or url.startswith('https://')\n",
        "\n",
        "def url_to_filename(url):\n",
        "    \"\"\"Converts a URL into a valid filename by removing the 'http://' or 'https://' part and non-filename characters.\"\"\"\n",
        "    # Remove 'http://' or 'https://'\n",
        "    url = re.sub(r'^https?://', '', url)\n",
        "    # Replace non-filename characters with underscores\n",
        "    filename = re.sub(r'\\W+', '_', url)\n",
        "    return filename\n"
      ],
      "metadata": {
        "id": "B3jYPK0BAOJG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U langchain_openai\n",
        "from langchain_openai import ChatOpenAI"
      ],
      "metadata": {
        "id": "51dECZlIUMHR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "llm = ChatOpenAI(temperature=0, model=MODEL)"
      ],
      "metadata": {
        "id": "2bCXmQ30GhEO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import create_extraction_chain\n",
        "\n",
        "schema = {\n",
        "    \"properties\": {\n",
        "        \"wardley_maps_concepts\": {\"type\": \"string\"},\n",
        "        \"strategy_concepts\": {\"type\": \"string\"},\n",
        "        \"wardley_doctrine\": {\"type\": \"string\"},\n",
        "        \"components\": {\"type\": \"string\"},\n",
        "    },\n",
        "    \"required\": [],\n",
        "}\n",
        "\n",
        "def extract(content: str, schema: dict):\n",
        "    print(\"Extracting content with LLM\")\n",
        "    return create_extraction_chain(schema=schema, llm=llm).run(content)"
      ],
      "metadata": {
        "id": "n2vTEpZrT2ZY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pprint\n",
        "import json\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "def scrape_with_playwright(url, schema):\n",
        "    loader = AsyncHtmlLoader(url)\n",
        "    doc = loader.load()\n",
        "    html2text = Html2TextTransformer()\n",
        "    doc_transformed = html2text.transform_documents(doc)\n",
        "\n",
        "    # Grab the first 1000 tokens of the site\n",
        "    splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
        "        chunk_size=10000, chunk_overlap=0\n",
        "    )\n",
        "    splits = splitter.split_documents(doc_transformed)\n",
        "\n",
        "    # Process the first split\n",
        "    if splits:\n",
        "        extracted_content = extract(schema=schema, content=splits[0].page_content)\n",
        "        print(extracted_content)\n",
        "        return extracted_content\n"
      ],
      "metadata": {
        "id": "fsByLI7OT3Re"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for url in urls:\n",
        "    filename_json = f\"{url_to_filename(url)}.json\"\n",
        "    file_path_json = os.path.join(WEBSITES_CONTENTS, filename_json)\n",
        "    filename_txt = f\"{url_to_filename(url)}.txt\"\n",
        "    file_path_txt = os.path.join(WEBSITES_CONTENTS, filename_txt)\n",
        "\n",
        "    if os.path.exists(file_path_txt):\n",
        "        print(f\"File already exists, skipping scraping: {file_path_txt}\")\n",
        "        continue\n",
        "\n",
        "    extracted_content = scrape_with_playwright(url, schema=schema)\n",
        "\n",
        "    # Check if extracted_content is not empty\n",
        "    if extracted_content:\n",
        "        # Flatten the list of dictionaries and remove empty values\n",
        "        flattened_content = {}\n",
        "        for content_dict in extracted_content:\n",
        "            if isinstance(content_dict, dict):\n",
        "                for key, value in content_dict.items():\n",
        "                    if value.strip():  # Filter out empty or whitespace-only values\n",
        "                        flattened_content[key] = value\n",
        "\n",
        "        pprint.pprint(flattened_content)\n",
        "\n",
        "        # Save to a separate file if there is content to save\n",
        "        if flattened_content:\n",
        "            print(\"Saving Contents      : \", file_path_json)\n",
        "            try:\n",
        "                with open(file_path_json, 'w', encoding='utf-8') as file:\n",
        "                    json.dump(flattened_content, file, ensure_ascii=False, indent=4)  # Save as JSON\n",
        "            except json.JSONEncodeError as e:\n",
        "                print(f\"Error saving JSON for {url}: {e}\")\n"
      ],
      "metadata": {
        "id": "HtuPvz1yf-FS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}